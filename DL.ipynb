{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras_self_attention\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "!pip install bangla-stemmer\n",
    "!pip install bnltk\n",
    "!pip install bnlp_toolkit\n",
    "!pip install tweet-preprocessor\n",
    "!pip install wordcloud\n",
    "!pip install https://github.com/JonathanRaiman/glove/archive/master.zip\n",
    "from bangla_stemmer.stemmer import stemmer\n",
    "from bnltk.stemmer import BanglaStemmer\n",
    "from bnltk.tokenize import Tokenizers\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "%matplotlib inline\n",
    "\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "import preprocessor as p\n",
    "import keras\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Conv1D,GRU,Concatenate\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\n",
    "from sklearn.metrics import average_precision_score,roc_auc_score, roc_curve, precision_recall_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from bnltk.tokenize import Tokenizers\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS =26000\n",
    "MAX_SEQ_LENGTH = 140\n",
    "EMBEDDING_DIM = 300\n",
    "newdf = []\n",
    "def clean_tweets(tweets,stem=True, stopword=True):\n",
    "    stopwordsBN = set(line.strip() for line in open('../input/bn-stopwords/stopwords.txt'))\n",
    "    stmr = stemmer.BanglaStemmer()\n",
    "    bn_stemmer = BanglaStemmer()\n",
    "    cleaned_tweets = []\n",
    "    a = \"\"\n",
    "    tweet = tweets\n",
    "    word_tokens = nltk.word_tokenize(tweet)\n",
    "    for w in range(len(word_tokens)):\n",
    "        a = word_tokens[w]            \n",
    "        a = \"\".join(i for i in a if  2432 <= ord(i) <= 2533 or ord(i)== 32)\n",
    "        tweet = re.sub(' +', ' ', a)\n",
    "        word_tokens[w] = tweet\n",
    "    filtered_sentence = None\n",
    "    if stem==True and stopwords==True:\n",
    "        filtered_sentence = [bn_stemmer.stem(w) for w in word_tokens if not w in stopwordsBN]\n",
    "    elif stem==True and stopword==False:\n",
    "        filtered_sentence = [bn_stemmer.stem(w) for w in word_tokens]\n",
    "    elif stem == False and stopword==True:\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stopwordsBN]\n",
    "    else:\n",
    "        filtered_sentence = [w for w in word_tokens]\n",
    "    tweet = ' '.join(filtered_sentence)        \n",
    "    tweet = str(tweet)\n",
    "    word_tokens = nltk.word_tokenize(tweet)\n",
    "    newdf.append(word_tokens)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def preprocess(corpus, stem = True, stopword=True):\n",
    "    newdf = []\n",
    "    ls = []\n",
    "    idd=[]\n",
    "    siz = []\n",
    "    dicti = {}\n",
    "    words=[]\n",
    "\n",
    "    for i in corpus:\n",
    "        j = clean_tweets(i,stem,stopword)\n",
    "        ls.append(j)\n",
    "    return ls\n",
    "\n",
    "\n",
    "def convertToTensor(texts):\n",
    "    print(\"Convert to tensor started\")\n",
    "    X = texts\n",
    "    tokenizer= Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    word_vector = tokenizer.texts_to_sequences(X)\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)\n",
    "    dictionary = word_index\n",
    "    np.save('dictionary.npy', dictionary) \n",
    "    t = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "    read_dictionary = np.load('./dictionary.npy',allow_pickle='TRUE').item()\n",
    "    t.word_index = read_dictionary\n",
    "    #t.fit_on_texts(X)\n",
    "    word_vector = t.texts_to_sequences(X)\n",
    "    MAX_SEQ_LENGTH = 140\n",
    "    input_tensor = pad_sequences(word_vector, maxlen=MAX_SEQ_LENGTH)\n",
    "    print(\"Convert to tensor ended\")\n",
    "    return input_tensor,word_index\n",
    "\n",
    "def Dataset_prepare(dataset=1):\n",
    "    print(\"tokenizer\")\n",
    "    #dataset == 1 for BEmoC\n",
    "    t = Tokenizers()\n",
    "    df = None\n",
    "    corpus = None\n",
    "    y = None\n",
    "    if dataset==1:\n",
    "        df=pd.read_excel(\"/kaggle/input/emotiondataset/bemoc.xlsx\")\n",
    "        corpus = df['TEXT'].to_list()\n",
    "        y=df['classes']\n",
    "        return corpus,y\n",
    "    elif dataset==3:\n",
    "        df=pd.read_csv(\"/kaggle/input/emotionmerged/merged.csv\")\n",
    "        corpus = df['text'].to_list()\n",
    "        y=df['class']\n",
    "        \n",
    "    else:\n",
    "        df=pd.read_csv(\"/kaggle/input/banglaemotion/train.csv\")\n",
    "        corpus = df['text'].to_list()\n",
    "        y=df['class']\n",
    "        df2 = pd.read_csv(\"/kaggle/input/banglaemotion/test.csv\")\n",
    "        corpus2 = df2['text'].to_list()\n",
    "        y2=df2['class']\n",
    "        corpus.extend(corpus2)\n",
    "        y = pd.concat([y,y2])\n",
    "    print(\"end tokenizer\")\n",
    "    return corpus,y\n",
    "\n",
    "\n",
    "def embedding_matrix_gen(word_index, embedding=\"fasttext\"):\n",
    "    print(\"embedding started\")\n",
    "    if str.lower(embedding)==\"word2vec\":\n",
    "        model = Word2Vec.load(\"../input/bnword2vec/bengali_word2vec.model\")\n",
    "        model.build_vocab(newdf, update=True)\n",
    "        model.train(newdf, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        not_found = 0\n",
    "        found = 0\n",
    "        EMBEDDING_DIM =300\n",
    "        embedding_matrix = np.zeros((MAX_NUM_WORDS, EMBEDDING_DIM))\n",
    "        for (word, idx) in word_index.items():\n",
    "            #print(idx)\n",
    "            try:\n",
    "                embedding_matrix[idx] =model.wv[word]\n",
    "                found=found+1\n",
    "            except:\n",
    "                embedding_matrix[idx] =embedding_matrix[0]\n",
    "                not_found = not_found+1\n",
    "                continue\n",
    "        print(\"embedding ended\")\n",
    "        return embedding_matrix,EMBEDDING_DIM\n",
    "    elif str.lower(embedding)==\"fasttext\":\n",
    "        print(len(newdf),newdf[0])\n",
    "        model = gensim.models.FastText.load_fasttext_format('../input/fasttext-and-glove/bengali_fasttext_wiki.bin')\n",
    "        #model.build_vocab(newdf, update=True)\n",
    "        #model.train(newdf, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        not_found = 0\n",
    "        found = 0\n",
    "        EMBEDDING_DIM =100\n",
    "        embedding_matrix = np.zeros((MAX_NUM_WORDS, EMBEDDING_DIM))\n",
    "        for (word, idx) in word_index.items():\n",
    "            #print(idx)\n",
    "            try:\n",
    "                embedding_matrix[idx] =model.wv[word]\n",
    "                found=found+1\n",
    "            except:\n",
    "                not_found = not_found+1\n",
    "                continue\n",
    "        print(\"embedding ended\")\n",
    "        return embedding_matrix,EMBEDDING_DIM \n",
    "\n",
    "    elif str.lower(embedding)==\"glove\":\n",
    "        glove_file = '../input/fasttext-and-glove/bn_glove.39M.300d.txt'\n",
    "        tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "        _ = glove2word2vec(glove_file, tmp_file)\n",
    "        model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "        EMBEDDING_DIM = 300\n",
    "        embedding_matrix = np.zeros((MAX_NUM_WORDS, EMBEDDING_DIM))\n",
    "        for (word, idx) in word_index.items():\n",
    "            #print(idx)\n",
    "            try:\n",
    "                embedding_matrix[idx] =model[word]\n",
    "            except:\n",
    "                embedding_matrix[idx] =embedding_matrix[0]\n",
    "                continue\n",
    "        print(\"embedding ended\")\n",
    "        return embedding_matrix,EMBEDDING_DIM        \n",
    "\n",
    "def dataset_splitting(dataset_id,input_tensor,y):\n",
    "    X_train=None\n",
    "    X_test=None\n",
    "    y_train=None\n",
    "    y_test = None\n",
    "    end = 5600\n",
    "    if dataset_id!=1:\n",
    "        end = 4700\n",
    "    if dataset_id==3:\n",
    "        end = 11240\n",
    "    print(len(input_tensor),print(len(y)))\n",
    "    X_train = input_tensor[0:end]\n",
    "    X_test = input_tensor[end:]\n",
    "    y_train = y[0:end].to_list()\n",
    "    y_test = y[end:].to_list()\n",
    "    encoder=LabelEncoder()\n",
    "    y_train=encoder.fit_transform(y_train)\n",
    "    y_train=pd.get_dummies(y_train).values\n",
    "    y_test=encoder.fit_transform(y_test)\n",
    "    y_test=pd.get_dummies(y_test).values\n",
    "    print(encoder.classes_)\n",
    "    return X_train,y_train, X_test,y_test\n",
    "\n",
    "def dataset_splitting_over(dataset_id,input_tensor,y):\n",
    "    X_train=None\n",
    "    X_test=None\n",
    "    y_train=None\n",
    "    y_test = None\n",
    "    end = 5600\n",
    "    if dataset_id!=1:\n",
    "        end = 4700\n",
    "    if dataset_id==3:\n",
    "        end = 11240\n",
    "    print(len(input_tensor),print(len(y)))\n",
    "    X_train = input_tensor[0:end]\n",
    "    X_test = input_tensor[end:]\n",
    "    y_train = y[0:end].to_list()\n",
    "    y_test = y[end:].to_list()\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_train_over, y_train_over = oversampler.fit_resample(X_train, y_train)\n",
    "    X_train = X_train_over\n",
    "    y_train = y_train_over\n",
    "    encoder=LabelEncoder()\n",
    "    y_train=encoder.fit_transform(y_train)\n",
    "    y_train=pd.get_dummies(y_train).values\n",
    "    y_test=encoder.fit_transform(y_test)\n",
    "    y_test=pd.get_dummies(y_test).values\n",
    "    print(encoder.classes_)\n",
    "    return X_train,y_train, X_test,y_test\n",
    "\n",
    "def train_entry(model,output_path,X_train,y_train,X_test,y_test,name,batch_size=64,epochs= 10):\n",
    "    print(len(X_test), len(y_test))\n",
    "    checkpoint = ModelCheckpoint(f'/kaggle/working/{name}_best.h5', verbose=1, monitor='val_accuracy',save_best_only=True, mode='max')\n",
    "    history=model.fit(X_train,y_train,batch_size=batch_size,epochs= epochs,validation_data=(X_test,y_test),callbacks=[checkpoint])\n",
    "    return model\n",
    "\n",
    "def base(embedding_matrix,EMBEDDING_DIM):\n",
    "    inp = Input(shape=(MAX_SEQ_LENGTH,))\n",
    "    x = Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, weights=[embedding_matrix])(inp)\n",
    "    x = Bidirectional(LSTM(100, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(x)\n",
    "    x = LSTM(100, return_sequences=True, dropout=0.25, recurrent_dropout=0.1)(x)\n",
    "    x= Conv1D(50,kernel_size=1,activation=\"relu\")(x)\n",
    "    x=Conv1D(250,kernel_size=1,activation=\"relu\")(x)\n",
    "    # att = SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,name='Attention',kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "    #                         bias_regularizer=keras.regularizers.l1(1e-4),\n",
    "    #                         attention_regularizer_weight=1e-4,attention_activation='relu')(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(250, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(6, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "def Attention_model(embedding_matrix,EMBEDDING_DIM):\n",
    "    inp = Input(shape=(MAX_SEQ_LENGTH,))\n",
    "    x = Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, weights=[embedding_matrix])(inp)\n",
    "    x = Bidirectional(LSTM(100, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(x)\n",
    "    x = LSTM(100, return_sequences=True, dropout=0.25, recurrent_dropout=0.1)(x)\n",
    "    x=Conv1D(50,kernel_size=1,activation=\"relu\")(x)\n",
    "    x=Conv1D(250,kernel_size=1,activation=\"relu\")(x)\n",
    "    att = SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,name='Attention',kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "                           bias_regularizer=keras.regularizers.l1(1e-4),\n",
    "                           attention_regularizer_weight=1e-4,attention_activation='relu')(x)\n",
    "    x = GlobalMaxPool1D()(att)\n",
    "    x = Dense(250, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(6, activation=\"softmax\")(x)\n",
    "    #from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    checkpoint = ModelCheckpoint('/kaggle/working/m1-{epoch:03d}-{acc:03f}-{val_accuracy:03f}.h5', verbose=1, monitor='val_accuracy',save_best_only=True, mode='max')\n",
    "    model1 = Model(inputs=inp, outputs=x)\n",
    "    model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model1.summary()\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "\n",
    "def evaluation(model_name,X_test,y_test,name):\n",
    "    print(model_name)\n",
    "    model1 = keras.models.load_model(model_name)\n",
    "    y_pred=model1.predict(X_test)\n",
    "    y_test_class=np.argmax(y_test,axis=1)\n",
    "    y_pred_class=np.argmax(y_pred,axis=1)\n",
    "    print(y_test_class)\n",
    "    cm=confusion_matrix(y_test_class,y_pred_class)\n",
    "    accuracy=accuracy_score(y_test_class,y_pred_class)\n",
    "    print(classification_report(y_test_class, y_pred_class, digits=5))\n",
    "    print(accuracy)\n",
    "    print(cm)\n",
    "    total = []\n",
    "    y_true=['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise']\n",
    "    for i in range(0,6):\n",
    "        total = 0 \n",
    "        for j in range(0,6):\n",
    "            total = total + cm[i][j]\n",
    "        print(f\"classwise-class accuracy{y_true[i]}: {cm[i][i]/total}\")\n",
    "\n",
    "    data = cm\n",
    "    df_cm = pd.DataFrame(data, columns=np.unique(y_true), index = np.unique(y_true))\n",
    "    sn.set(font_scale=2)#for label size\n",
    "    sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 24}, fmt=\"d\")\n",
    "    plt.savefig(f'{name}_Cm.pdf', format='pdf', dpi=300)\n",
    "    plt.savefig(f'{name}_Cm.png', format='png', dpi=300)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiLSTM-CNN, Embedding-fastText, Datset- BEMoC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id =1\n",
    "stem = True\n",
    "stopwords = True\n",
    "embedding=\"fasttext\"\n",
    "model_name = \"base\"\n",
    "output_path=\"./\"\n",
    "batch_size = 64\n",
    "epochs =20\n",
    "name=\"fastTextbase\"\n",
    "corpus,y = Dataset_prepare(dataset_id)\n",
    "texts = preprocess(corpus,stem,stopwords)\n",
    "input_tensor,word_index = convertToTensor(texts)\n",
    "embedding_matrix, EMBEDDING_DIM = embedding_matrix_gen(embedding=\"fasttext\",word_index=word_index)\n",
    "X_train,y_train, X_test,y_test = dataset_splitting(dataset_id,input_tensor,y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "model = None\n",
    "if model_name==\"base\":\n",
    "    model = base(embedding_matrix,EMBEDDING_DIM)\n",
    "history = train_entry(model,output_path,X_train,y_train, X_val,y_val, name, batch_size=batch_size,epochs=epochs)\n",
    "model.save(\"/kaggle/working/final.h5\")\n",
    "evaluation(f\"/kaggle/working/{name}_best.h5\", X_test,y_test,name)  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiLSTM-CNN, Embedding-word2vec, Datset- BEMoC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id =1\n",
    "stem = True\n",
    "stopwords = True\n",
    "embedding=\"word2vec\"\n",
    "model_name = \"base\"\n",
    "output_path=\"./\"\n",
    "batch_size = 64\n",
    "epochs =20\n",
    "name=\"word2vec-base\"\n",
    "corpus,y = Dataset_prepare(dataset_id)\n",
    "texts = preprocess(corpus,stem,stopwords)\n",
    "input_tensor,word_index = convertToTensor(texts)\n",
    "embedding_matrix, EMBEDDING_DIM = embedding_matrix_gen(embedding=\"fasttext\",word_index=word_index)\n",
    "X_train,y_train, X_test,y_test = dataset_splitting(dataset_id,input_tensor,y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "model = None\n",
    "if model_name==\"base\":\n",
    "    model = base(embedding_matrix,EMBEDDING_DIM)\n",
    "history = train_entry(model,output_path,X_train,y_train, X_val,y_val,name, batch_size=batch_size,epochs=epochs)\n",
    "model.save(\"/kaggle/working/final.h5\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiLSTM-CNN-Attention, Embedding-fastText, Datset- BEMoC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id =1\n",
    "stem = True\n",
    "stopwords = True\n",
    "embedding=\"fasttext\"\n",
    "model_name = \"attention\"\n",
    "output_path=\"./\"\n",
    "batch_size = 64\n",
    "epochs =20\n",
    "name=\"attention-fastText\"\n",
    "corpus,y = Dataset_prepare(dataset_id)\n",
    "texts = preprocess(corpus,stem,stopwords)\n",
    "input_tensor,word_index = convertToTensor(texts)\n",
    "embedding_matrix, EMBEDDING_DIM = embedding_matrix_gen(embedding=\"fasttext\",word_index=word_index)\n",
    "X_train,y_train, X_test,y_test = dataset_splitting(dataset_id,input_tensor,y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "model = None\n",
    "if model_name==\"attention\":\n",
    "    model = Attention_model(embedding_matrix,EMBEDDING_DIM)\n",
    "history = train_entry(model,output_path,X_train,y_train, X_val,y_val,name, batch_size=batch_size,epochs=epochs)\n",
    "model.save(\"/kaggle/working/final.h5\")\n",
    "y_pred=history.predict(X_test)\n",
    "y_test_class=np.argmax(y_test,axis=1)\n",
    "y_pred_class=np.argmax(y_pred,axis=1)\n",
    "print(y_test_class)\n",
    "cm=confusion_matrix(y_test_class,y_pred_class)\n",
    "accuracy=accuracy_score(y_test_class,y_pred_class)\n",
    "print(classification_report(y_test_class, y_pred_class, digits=5))\n",
    "print(accuracy)\n",
    "print(cm)\n",
    "total = []\n",
    "y_true=['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise']\n",
    "for i in range(0,6):\n",
    "    total = 0 \n",
    "    for j in range(0,6):\n",
    "        total = total + cm[i][j]\n",
    "    print(f\"classwise-class accuracy{y_true[i]}: {cm[i][i]/total}\")\n",
    "\n",
    "y_true=['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise']\n",
    "data = cm\n",
    "df_cm = pd.DataFrame(data, columns=np.unique(y_true), index = np.unique(y_true))\n",
    "sn.set(font_scale=2)#for label size\n",
    "sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 24}, fmt=\"d\")\n",
    "plt.savefig(f'{name}_Cm.pdf', format='pdf', dpi=300)\n",
    "plt.savefig(f'{name}_Cm.png', format='png', dpi=300)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiLSTM-CNN-Attention, Embedding-fastText, Datset- BEMoC  Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id =1\n",
    "stem = True\n",
    "stopwords = True\n",
    "embedding=\"fasttext\"\n",
    "model_name = \"attention\"\n",
    "output_path=\"./\"\n",
    "batch_size = 64\n",
    "epochs =20\n",
    "name=\"attention-fastText-over\"\n",
    "corpus,y = Dataset_prepare(dataset_id)\n",
    "texts = preprocess(corpus,stem,stopwords)\n",
    "input_tensor,word_index = convertToTensor(texts)\n",
    "embedding_matrix, EMBEDDING_DIM = embedding_matrix_gen(embedding=\"fasttext\",word_index=word_index)\n",
    "X_train,y_train, X_test,y_test = dataset_splitting_over(dataset_id,input_tensor,y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "model = None\n",
    "if model_name==\"attention\":\n",
    "    model = Attention_model(embedding_matrix,EMBEDDING_DIM)\n",
    "history = train_entry(model,output_path,X_train,y_train, X_val,y_val,name, batch_size=batch_size,epochs=epochs)\n",
    "model.save(\"/kaggle/working/final.h5\")\n",
    "evaluation(f\"/kaggle/working/{name}_best.h5\", X_test,y_test,name) \n",
    "y_pred=history.predict(X_test)\n",
    "y_test_class=np.argmax(y_test,axis=1)\n",
    "y_pred_class=np.argmax(y_pred,axis=1)\n",
    "print(y_test_class)\n",
    "cm=confusion_matrix(y_test_class,y_pred_class)\n",
    "accuracy=accuracy_score(y_test_class,y_pred_class)\n",
    "print(classification_report(y_test_class, y_pred_class, digits=5))\n",
    "print(accuracy)\n",
    "print(cm)\n",
    "total = []\n",
    "y_true=['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise']\n",
    "for i in range(0,6):\n",
    "    total = 0 \n",
    "    for j in range(0,6):\n",
    "        total = total + cm[i][j]\n",
    "    print(f\"classwise-class accuracy{y_true[i]}: {cm[i][i]/total}\")\n",
    "\n",
    "y_true=['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise']\n",
    "data = cm\n",
    "df_cm = pd.DataFrame(data, columns=np.unique(y_true), index = np.unique(y_true))\n",
    "sn.set(font_scale=2)#for label size\n",
    "sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 24}, fmt=\"d\")\n",
    "plt.savefig(f'{name}_Cm.pdf', format='pdf', dpi=300)\n",
    "plt.savefig(f'{name}_Cm.png', format='png', dpi=300) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiLSTM-CNN-Attention, Embedding-fastText, Datset- BanglaEmotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id =2\n",
    "stem = True\n",
    "stopwords = True\n",
    "embedding=\"fasttext\"\n",
    "model_name = \"attention\"\n",
    "output_path=\"./\"\n",
    "batch_size = 64\n",
    "epochs =20\n",
    "name=\"attention-fastText-banglaemotion\"\n",
    "corpus,y = Dataset_prepare(dataset_id)\n",
    "texts = preprocess(corpus,stem,stopwords)\n",
    "print(texts[3])\n",
    "input_tensor,word_index = convertToTensor(texts)\n",
    "print(input_tensor)\n",
    "embedding_matrix, EMBEDDING_DIM = embedding_matrix_gen(embedding=\"fasttext\",word_index=word_index)\n",
    "X_train,y_train, X_test,y_test = dataset_splitting(dataset_id,input_tensor,y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "model = None\n",
    "if model_name==\"attention\":\n",
    "    model = Attention_model(embedding_matrix,EMBEDDING_DIM)\n",
    "history = train_entry(model,output_path,X_train,y_train, X_val,y_val,name, batch_size=batch_size,epochs=20)\n",
    "history.save(\"/kaggle/working/final.h5\")\n",
    "y_pred=history.predict(X_test)\n",
    "y_test_class=np.argmax(y_test,axis=1)\n",
    "y_pred_class=np.argmax(y_pred,axis=1)\n",
    "print(y_test_class)\n",
    "cm=confusion_matrix(y_test_class,y_pred_class)\n",
    "accuracy=accuracy_score(y_test_class,y_pred_class)\n",
    "print(classification_report(y_test_class, y_pred_class, digits=5))\n",
    "print(accuracy)\n",
    "print(cm)\n",
    "total = []\n",
    "y_true=['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise']\n",
    "for i in range(0,6):\n",
    "    total = 0 \n",
    "    for j in range(0,6):\n",
    "        total = total + cm[i][j]\n",
    "    print(f\"classwise-class accuracy{y_true[i]}: {cm[i][i]/total}\")\n",
    "\n",
    "data = cm\n",
    "df_cm = pd.DataFrame(data, columns=np.unique(y_true), index = np.unique(y_true))\n",
    "sn.set(font_scale=2)#for label size\n",
    "sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 24}, fmt=\"d\")\n",
    "plt.savefig(f'{name}_Cm.pdf', format='pdf', dpi=300)\n",
    "plt.savefig(f'{name}_Cm.png', format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiLSTM-CNN-Attention, Embedding-fastText, Datset- Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id =3\n",
    "stem = True\n",
    "stopwords = True\n",
    "embedding=\"fasttext\"\n",
    "model_name = \"attention\"\n",
    "output_path=\"./\"\n",
    "batch_size = 64\n",
    "epochs =20\n",
    "name=\"attention-fastText-merged\"\n",
    "corpus,y = Dataset_prepare(dataset_id)\n",
    "texts = preprocess(corpus,stem,stopwords)\n",
    "print(texts[3])\n",
    "input_tensor,word_index = convertToTensor(texts)\n",
    "print(input_tensor)\n",
    "embedding_matrix, EMBEDDING_DIM = embedding_matrix_gen(embedding=\"fasttext\",word_index=word_index)\n",
    "X_train,y_train, X_test,y_test = dataset_splitting(dataset_id,input_tensor,y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "model = None\n",
    "if model_name==\"attention\":\n",
    "    model = Attention_model(embedding_matrix,EMBEDDING_DIM)\n",
    "history = train_entry(model,output_path,X_train,y_train, X_val,y_val,name, batch_size=batch_size,epochs=20)\n",
    "history.save(\"/kaggle/working/final.h5\")\n",
    "y_pred=history.predict(X_test)\n",
    "y_test_class=np.argmax(y_test,axis=1)\n",
    "y_pred_class=np.argmax(y_pred,axis=1)\n",
    "print(y_test_class)\n",
    "cm=confusion_matrix(y_test_class,y_pred_class)\n",
    "accuracy=accuracy_score(y_test_class,y_pred_class)\n",
    "print(classification_report(y_test_class, y_pred_class, digits=5))\n",
    "print(accuracy)\n",
    "print(cm)\n",
    "total = []\n",
    "y_true=['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise']\n",
    "for i in range(0,6):\n",
    "    total = 0 \n",
    "    for j in range(0,6):\n",
    "        total = total + cm[i][j]\n",
    "    print(f\"classwise-class accuracy{y_true[i]}: {cm[i][i]/total}\")\n",
    "\n",
    "data = cm\n",
    "df_cm = pd.DataFrame(data, columns=np.unique(y_true), index = np.unique(y_true))\n",
    "sn.set(font_scale=2)#for label size\n",
    "sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 24}, fmt=\"d\")\n",
    "plt.savefig(f'{name}_Cm.pdf', format='pdf', dpi=300)\n",
    "plt.savefig(f'{name}_Cm.png', format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
